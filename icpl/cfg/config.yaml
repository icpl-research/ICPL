defaults:
  - _self_
  - env: humanoid
  - override hydra/launcher: local
  - override hydra/output: local

hydra:
  job:
    chdir: True

# LLM parameters
model: gpt-4o  # LLM model (other options are gpt-4, gpt-4-0613, gpt-3.5-turbo-16k-0613)
temperature: 1.0
suffix: GPT  # suffix for generated files (indicates LLM model)
num_envs: ''
checkpoint: ''

# feedback
preference_type: 'simple_human'
use_env_feedback: True
use_human_feedback: False
use_sparse_reward: False
user_list_only: True 
use_history_diff: True 
use_reward_trace: True
render: True
open_loop: False

# Eureka parameters
iteration: 5 # how many iterations of Eureka to run
sample: 6 # number of Eureka samples to generate per iteration
max_iterations: 3000 # RL Policy training iterations (decrease this to make the feedback loop faster)
num_eval: 5 # number of evaluation episodes to run for the final reward
capture_video: False # whether to capture policy rollout videos

# Weights and Biases
use_wandb: False # whether to use wandb for logging
wandb_username: "" # wandb username if logging with wandb
wandb_project: "" # wandb project if logging with wandb